# Efficient Parallel Deep Reinforcement Learning Framework
by Yixiu Zhao (yixiuz) and Shangda Li (shangdal)

## Summary
We are going to optimize a parallel deep reinforcement learning framework known as PAAC.

## Background
From exceeding humans on Atari games to mastering the game of Go, deep reinforcement learning has been a huge success in recent years. These algorithms use a variety of methods that can be classified into many different categories, two of which are on-policy and off-policy learning. On-policy learning means that the agent always takes the observations generated by the most up-to-date policy to update its network. In an off-policy framework, the algorithm does not have to make sure it gets the latest observations, therefore it is free to build up a pool of memory to reuse and learn from. However, the off-policy framework suffers severely from the problem of non-convergence, which is the main problem all reinforcement learning algorithms have to face. 	The problem of non-convergence stems from the fact that the gradient descent algorithm is only proven to work with data that is independent and identically distributed (i.i.d.). However, in the reinforcement learning setting, there is a spatial and temporal relationship in the agent’s experiences, so the i.i.d. assumption falls apart, and there is no proof for convergence. Intuitively, off-policy algorithms may suffer from this problem more because there is less correspondence between the observation and current policy parameters. Therefore in some cases it might be desirable to use an on-policy method to obtain stability in training.

On the other hand, to obtain a set data that is more close to i.i.d., multiple agents acting in parallel environments can be introduced to get a more spread-out distribution of data. Recently there has been many novel parallel reinforcement learning frameworks such as GA3C that optimizes a specific algorithm (namely A3C) so that it can be run efficiently on a combination of CPUs and GPUs. However these algorithms typically only support the off-policy framework, which might have disadvantages as mentioned before. PAAC is a novel framework that supports both on-policy and off-policy learning.

Our goal in this project is to optimize the PAAC algorithm to decrease the idle time of the processors to increase the total throughput of the PAAC algorithm, while preserving its compatibility with on-policy methods. We plan to achieve this goal by using pipelining methods to control the workflow so that idle time is minimized.

## Challenge
PAAC is a very general algorithm, which means it is relatively simple and hard to further optimize. Also we plan to keep the algorithm’s compatibility with on-policy methods, which means there is less freedom in changing the algorithm in order to hide latency.

The combination of CPUs and GPUs in the implementation of the algorithm can also be a challenge, as we have to face many technical problems like the speed of the interconnection between the CPUs and GPUs.

## Resources
PAAC paper: https://arxiv.org/pdf/1705.04862.pdf

Sourcecode: https://github.com/Alfredvc/paac

We will be using the processors in XuLab@CMU: https://sites.google.com/view/xulab/home


## Goals/Deliverables
The most important metrics of evaluation are training per second (TPS), prediction per second (PPS) and the overall score during training. There are two main criteria that we want to evaluate. Firstly we want the network to perform faster with the current number of workers (measured in TPS and PPS). Secondly we want to use more workers at roughly the same TPS and PPS and see if it leads to faster convergence (measured in score), because the experiences are drawn from more randomly distributed sources.

We hope to achieve at least a 1.5x speedup over the original paper in terms of training throughput, and we are happy with any improvement in terms of score vs. number of training data.

## Platform
We are using the python Tensorflow library for evaluation of deep neural networks within the algorithm, as it is the same platform used by the original paper.

## Schedule
Week 1: Read the paper and code from the original authors.
Week 2: Try to get the original code running on our machines.
Week 3: Try to get the original code running on our machines and do minor adjustments.
Week 4: Try to improve the original code using methods using pipelining.
Week 5: Try more optimization tricks if the methods don’t work. Start doing experiments.
Week 6: Work on the final writeup.

## References
“EFFICIENT PARALLEL METHODS FOR DEEP REINFORCEMENT LEARNING”, 
arXiv:1705.04862 [cs.LG], https://arxiv.org/abs/1705.04862
