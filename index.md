## Efficient Parallel Deep Reinforcement Learning Framework
by Yixiu Zhao (yixiuz) and Shangda Li (shangdal)

### Summary
We are going to optimize a parallel deep reinforcement learning framework known as PAAC.

### Background
From exceeding humans on Atari games to mastering the game of Go, deep reinforcement learning has been a huge success in recent years. These algorithms use a variety of methods that can be classified into many different categories, two of which are on-policy and off-policy learning. On-policy learning means that the agent always takes the observations generated by the most up-to-date policy to update its network. In an off-policy framework, the algorithm does not have to make sure it gets the latest observations, therefore it is free to build up a pool of memory to reuse and learn from. However, the off-policy framework suffers severely from the problem of non-convergence, which is the main problem all reinforcement learning algorithms have to face. 	The problem of non-convergence stems from the fact that the gradient descent algorithm is only proven to work with data that is independent and identically distributed (i.i.d.). However, in the reinforcement learning setting, there is a spatial and temporal relationship in the agent’s experiences, so the i.i.d. assumption falls apart, and there is no proof for convergence. Intuitively, off-policy algorithms may suffer from this problem more because there is less correspondence between the observation and current policy parameters. Therefore in some cases it might be desirable to use an on-policy method to obtain stability in training.

On the other hand, to obtain a set data that is more close to i.i.d., multiple agents acting in parallel environments can be introduced to get a more spread-out distribution of data. Recently there has been many novel parallel reinforcement learning frameworks such as GA3C that optimizes a specific algorithm (namely A3C) so that it can be run efficiently on a combination of CPUs and GPUs. However these algorithms typically only support the off-policy framework, which might have disadvantages as mentioned before. PAAC is a novel framework that supports both on-policy and off-policy learning.

Our goal in this project is to optimize the PAAC algorithm to decrease the idle time of the processors to increase the total throughput of the PAAC algorithm, while preserving its compatibility with on-policy methods. We plan to achieve this goal by using pipelining methods to control the workflow so that idle time is minimized.


Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for

```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).

### Jekyll Themes

Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/iharryharryli/15418-Final-Project/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

### Support or Contact

Having trouble with Pages? Check out our [documentation](https://help.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and we’ll help you sort it out.
